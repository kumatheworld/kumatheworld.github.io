<!DOCTYPE HTML>
<html>
  <head>
  <link rel="stylesheet" href="../css/style.css">
  <link rel="icon" type="image/png" href="../images/kumatheworld.png">
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/themes/prism.min.css" integrity="sha256-77qGXu2p8NpfcBpTjw4jsMeQnz0vyh74f5do0cWjQ/Q=" crossorigin="anonymous" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js" integrity="sha256-HWJnMZHGx7U1jmNfxe4yaQedmpo/mtxWSIXvcJkLIf4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js" integrity="sha256-zXSwQE9cCZ8HHjjOoy6sDGyl5/3i2VFAxU8XxJWfhC0=" crossorigin="anonymous"></script>
  <title>My backprop for conv layers is wrong but works, why?</title>
  </head>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <h1>My backprop for conv layers is wrong but works, why?</h1>
        Last updated: Dec. 5th, 2019
        <hr>
        <p>
          Recently I wrote a CNN in NumPy from scratch, without resorting to any deep learning framework. It was exciting to derive the backpropagation formula for several types of layers such as linear layers, max pooling layers, etc. But I ran into something very weird while I was implementing the backprop algorithm for convolutional layers.
        </p>
        <p>
          Here we consider a basic convolutional layer where stride=1, padding=0 and so on. Let \( w \) be the weight tensor of the layer and \( x, y \) be an input and the corresponding output respectively. Leaving out details, the relationship among those variables can be put very simply as follows.
          \[
            y = x \otimes w
          \]
          where \( \otimes \) stands for the cross <b>correlation</b> operation. The bias term is ommited here.
        </p>
        <p>
          So the above is the forward propagation formula. How about the backprop? Actually it's not hard either. Given the derivative of the loss w.r.t. \( y \) (let's denote it \( \partial y \)), the derivative of the loss w.r.t. \( x \) is given by the following formula.
          \[
            \partial x = [\partial y]_0 * w
          \]
          where \( [\partial y]_0 \) is a zero-padded \( \partial y \) and \( * \) is the <b>convolution</b> operation, which is different than the correlation operation above. It's just like taking a correlation with a <b>flipped</b> \( w \). And here's an implementation for this calculation in Python.
        <pre><code class="language-python">dx = np.concatenate([
  correlate(
      np.pad(dy[i], ((0,0), (kh-1,kh-1), (kw-1,kw-1))),
      np.flip(w[:, j], (1,2)),
      'valid'
  ) for i in range(dy.shape[0]) for j in range(cin)
]).reshape(x.shape)</code></pre>
          It looks a little bit intricate due to the batch size things and stuff. But basically it simply calculates the convolution between \( y \) and \( w \).
        </p>
        <p>
          Actually, I got the convolution formula wrong at first and I thought it was the <b>correlation</b> operation to be performed, not the convolution. Here's the formula and code.
          \[
            \partial x = [\partial y]_0 \otimes w.
          \]
          <pre><code class="language-python">dx = np.concatenate([
  correlate(
      np.pad(dy[i], ((0,0), (kh-1,kh-1), (kw-1,kw-1))),
      w[:, j],
      'valid'
  ) for i in range(dy.shape[0]) for j in range(cin)
]).reshape(x.shape)</code></pre>
          Do you think it's gonna work? I didn't think so, but when I tried training a CNN with 2 convolutional layers which used the wrong backprop formula, the training process was as successful as one with the right backprop formula. Here's the loss curve.
          <img src="../images/cnn_loss_plot.png">
        </p>
        <p>
          I still can't figure out why it worked. Chances are that the filters are flip-invariant but it seems unlikely. And there's another mystery. The loss decreased immediately when it was implemented in PyTorch (the blue line in the figure), but it wasn't the case for the handmade CNNs. They did't learn much in the first epoch. Maybe it was because of bad initializations, but I used the one of Xavier so it shouldn't affect the result that much. I hope I can understand this weird loss behavior in the future.
        </p>
        <p>
          Code is available at <a href="https://github.com/kumatheworld/nn-playground">my GitHub repository</a>.
        </p>

      </td>
    </tr>
  </table>
  </body>
</html>
